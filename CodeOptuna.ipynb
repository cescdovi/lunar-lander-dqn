{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "958cbb11",
   "metadata": {},
   "source": [
    "# librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea7fbb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import plotly\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from typing import Optional\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.exceptions import MlflowException\n",
    "\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "from optuna.visualization import plot_parallel_coordinate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0b582a",
   "metadata": {},
   "source": [
    "# init_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed4e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entorno\n",
    "ENV_NAME                 = \"LunarLander-v3\"\n",
    "SEED                     = 42\n",
    "\n",
    "MEM_LENGTH               = 2048\n",
    "TARGET_UPDATE            = 1000       # cada cuántos pasos sincronizar target_net\n",
    "EPSILON_START            = 1.0\n",
    "EPSILON_END              = 0.01\n",
    "EPS_DECAY_STEPS          = 300000        # pasos para llegar a EPS_END\n",
    "\n",
    "# Entrenamiento\n",
    "MAX_EPISODES             = 5000\n",
    "MAX_STEPS_PER_EPISODE    = 1000\n",
    "\n",
    "TRAINING_RATIO           = 4\n",
    "EMA_RATIO                = 0.01\n",
    "TAU                      = 0.125\n",
    "EP                       = 2 #cada cuantos episodios mostrar rendimiento\n",
    "STOPPING_REWARD_CRITERI  = 200\n",
    "\n",
    "#optuna\n",
    "OPTUNA_TRIALS            = 8\n",
    "GAMMAS                   = [0.9, 0.99]\n",
    "LEARNING_RATES           = [3e-4, 5e-3]\n",
    "BATCH_SIZES              = [16, 32, 64, 128]\n",
    "N_LAYERS                 = [1,2,3,4]\n",
    "N_NEURONS                = [16,32,64,128]\n",
    "\n",
    "# Rutas\n",
    "MLFLOW_TRACKING_URI      = None   # Si se quiere apuntar a servidor remoto, p.ej. \"http://localhost:5000\"\n",
    "MLFLOW_EXPERIMENT        = \"LunarLander_DQN\"\n",
    "\n",
    "#device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab0437a",
   "metadata": {},
   "source": [
    "Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96a38cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "#leer dimensiones automáticas\n",
    "state_dim  = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e081784",
   "metadata": {},
   "source": [
    "# replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebdbb6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, \n",
    "                 capacity: int = MEM_LENGTH):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Almacena una experiencia en el buffer de experiencias.\n",
    "        \"\"\"\n",
    "        self.memory.append(Transition(state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, \n",
    "               batch_size: int):\n",
    "        \"\"\"Devuelve un batch aleatorio de experiencias.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Devuelve longitud del buffer\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47c319",
   "metadata": {},
   "source": [
    "# neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57491a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim: int, \n",
    "                 output_dim: int,\n",
    "                 n_layers: int,         \n",
    "                 n_neurons: int):       \n",
    "\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Primera capa\n",
    "        in_features = input_dim\n",
    "        out_features = n_neurons\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            in_features = out_features\n",
    "            out_features = max(1, out_features // 2)  # Reducimos a la mitad, mínimo 1 neurona\n",
    "\n",
    "        # Capa final\n",
    "        layers.append(nn.Linear(in_features, output_dim))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers[:-1]:  \n",
    "            x = F.relu(layer(x))\n",
    "        return self.layers[-1](x) \n",
    "\n",
    "def learning_transfer(Q_model: nn.Module, \n",
    "                      T_model: nn.Module, \n",
    "                      tau: float):\n",
    "    \n",
    "    q_sd = Q_model.state_dict()\n",
    "    t_sd = T_model.state_dict()\n",
    "    for k in q_sd.keys():\n",
    "        t_sd[k].copy_(tau * q_sd[k] + (1.0 - tau) * t_sd[k])\n",
    "    T_model.load_state_dict(t_sd)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d753c",
   "metadata": {},
   "source": [
    "# policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8bdcaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy:\n",
    "    def __init__(self,\n",
    "                 action_dim: int,\n",
    "                 q_model: torch.nn.Module,\n",
    "                 start: float = EPSILON_START, \n",
    "                 end: float = EPSILON_END, \n",
    "                 decay_steps: int = EPS_DECAY_STEPS):\n",
    "        \"\"\"\n",
    "        Inicializa la política epsilon-greedy.\n",
    "        \"\"\"\n",
    "        self.start = start              \n",
    "        self.end = end                  \n",
    "        self.decay_steps = decay_steps  \n",
    "        self.steps_done = 0  \n",
    "        self.q_model = q_model            \n",
    "        self.action_dim = action_dim    \n",
    "\n",
    "    def get_epsilon(self) -> float:\n",
    "        # Decaimiento lineal:\n",
    "        frac = max(0, (self.decay_steps - self.steps_done)) / self.decay_steps\n",
    "        return self.end + (self.start - self.end) * frac\n",
    "\n",
    "    def select_action(self, state: torch.Tensor) -> int:\n",
    "        eps_threshold = self.get_epsilon()\n",
    "        self.steps_done += 1\n",
    "\n",
    "        if random.random() < eps_threshold:\n",
    "            # Explorar\n",
    "            action = random.randrange(self.action_dim)\n",
    "        else:\n",
    "            # Explotar\n",
    "            with torch.no_grad():\n",
    "                #state = state.unsqueeze(0).to(DEVICE)\n",
    "                q_values = self.q_model(state)\n",
    "                action = q_values.argmax(dim=1).item()\n",
    "                # action = q_values.max(1)[1].item() \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66950f0",
   "metadata": {},
   "source": [
    "# optimize_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c99c2b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "def optimize_model(samples: list,\n",
    "                   q_model: torch.nn.Module,\n",
    "                   t_model: torch.nn.Module,\n",
    "                   optimizer: torch.optim.Optimizer,\n",
    "                   gamma: float,\n",
    "                   batch_size: int\n",
    "                   ):\n",
    "    \n",
    "    if len(samples) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = samples\n",
    "\n",
    "    # Convertir lista de np.ndarray → un tensor (batch_size x state_dim)\n",
    "    \n",
    "    state_batch = torch.stack([t.state for t in batch], dim=0).squeeze(1).to(DEVICE) #(batch_size, n_states)\n",
    "    next_state_batch = torch.stack([t.next_state for t in batch], dim=0).squeeze(1).to(DEVICE)  #(batch_size, n_states)\n",
    "\n",
    "    #Convertir acciones/rew/done a tensores con las formas adecuadas\n",
    "    action_batch = torch.tensor([t.action for t in batch], dtype=torch.int64, device=DEVICE).unsqueeze(1)  # (batch_size, 1)\n",
    "    reward_batch = torch.tensor([t.reward for t in batch], dtype=torch.float32, device=DEVICE).unsqueeze(1) # (batch_size, 1)\n",
    "    done_batch   = torch.tensor([t.done for t in batch], dtype=torch.float32, device=DEVICE).unsqueeze(1) # (batch_size, 1)\n",
    "        \n",
    "\n",
    "    #Predicciones actuales Q(s_t, a_t; theta)\n",
    "    state_action_values = q_model(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # 4.2. Double DQN: Selección con q_model, evaluación con T_MODEL\n",
    "    with torch.no_grad():\n",
    "        # Seleccionamos la acción que maximiza Q con la red online\n",
    "        next_state_best_actions = q_model(next_state_batch).max(1)[1].unsqueeze(1)  # (batch_size x 1)\n",
    "        # Evaluamos esas acciones con la red objetivo\n",
    "        next_state_values = t_model(next_state_batch).gather(1, next_state_best_actions)\n",
    "        # Si es terminal, anulamos valor futuro\n",
    "        next_state_values = next_state_values * (1 - done_batch)\n",
    "        # Construimos el target\n",
    "        target_values = reward_batch + (gamma * next_state_values)\n",
    "\n",
    "    # 4.3. Cálculo de pérdida MSE y optimización\n",
    "    loss = F.mse_loss(state_action_values, target_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Clipping de gradientes (opcional, pero generalmente útil)\n",
    "    # for param in q_model.parameters():\n",
    "    #     param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230cc84c",
   "metadata": {},
   "source": [
    "# eval_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfe8423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env: gym.Env,\n",
    "             state_dim: int,\n",
    "             action_dim: int,\n",
    "             n_layers: int,\n",
    "             n_neurons: int,\n",
    "             q_model: Optional[nn.Module] = None,\n",
    "             model_path: Optional[str] = None,\n",
    "             num_episodes: int = 5,\n",
    "             render: bool = False) -> float:\n",
    "    \n",
    "    if q_model is None:\n",
    "        assert model_path is not None, \"O debes pasar q_model, o un model_path\"\n",
    "        q_model = DQN(input_dim = state_dim, output_dim = action_dim, n_layers = n_layers, n_neurons = n_neurons)\n",
    "        q_model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    q_model.to(DEVICE).eval()\n",
    "\n",
    "\n",
    "    rewards_por_episodio = [] #acumula rewards de todos los episodios para calcualr promedio por episodio\n",
    "\n",
    "    for ep in range(1, num_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0.0 #reward de episodio actual\n",
    "\n",
    "        for time_step in range(1, MAX_STEPS_PER_EPISODE):\n",
    "            # 3) Seleccionamos acción greedy: argmax Q(s, a)\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = q_model(state_tensor)\n",
    "            action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "            # 4) Interactuamos con el entorno\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "            state = next_state\n",
    "        rewards_por_episodio.append(total_reward)\n",
    "        #print(f\"[Eval] Episodio {ep}/{num_episodes} → Recompensa: {total_reward:.2f}\")\n",
    "    env.close()\n",
    "    reward_avg = np.mean(rewards_por_episodio)\n",
    "    #print(f\"[Eval] Recompensa media en {num_episodes} episodios: {reward_avg:.2f}\\n\")\n",
    "    return reward_avg, rewards_por_episodio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31682897",
   "metadata": {},
   "source": [
    "# build_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "979fb45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(state_dim: int,\n",
    "                action_dim: int,\n",
    "                n_layers: int, \n",
    "                n_neurons: int,\n",
    "                learning_rate: int) -> dict:\n",
    "    \"\"\"\n",
    "    Construye y devuelve un dict con:\n",
    "      - 'q_online'   : red Q principal (PyTorch Module)\n",
    "      - 'q_target'   : red target (idéntica a q_online al inicio, en modo eval)\n",
    "      - 'optimizer'  : optimizador Adam sobre los parámetros de q_online\n",
    "      - 'buffer'     : ReplayBuffer() con capacidad MEM_LENGTH por defecto\n",
    "      - 'policy'     : EpsilonGreedyPolicy( q_online, action_dim, start=EPSILON_START,\n",
    "                    end=EPSILON_END, decay_steps=EPS_DECAY_STEPS )\n",
    "      - 'total_steps': contador global de pasos, inicializado a 0\n",
    "    \"\"\"\n",
    "    q_online = DQN(input_dim = state_dim, output_dim = action_dim, n_layers = n_layers, n_neurons = n_neurons).to(DEVICE)\n",
    "    q_target = DQN(input_dim=state_dim, output_dim=action_dim, n_layers = n_layers, n_neurons = n_neurons).to(DEVICE)\n",
    "    q_target.load_state_dict(q_online.state_dict())\n",
    "    q_target.eval()\n",
    "\n",
    "    optimizer = torch.optim.Adam(q_online.parameters(), lr=learning_rate)\n",
    "    buffer = ReplayBuffer()\n",
    "    policy = EpsilonGreedyPolicy(q_model = q_online,\n",
    "                                 action_dim = action_dim,\n",
    "                                 start = EPSILON_START,\n",
    "                                 end = EPSILON_END,\n",
    "                                 decay_steps = EPS_DECAY_STEPS)\n",
    "    return {\n",
    "        \"q_online\": q_online,\n",
    "        \"q_target\": q_target,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"buffer\": buffer,\n",
    "        \"policy\": policy,\n",
    "        \"total_steps\": 0\n",
    "    }\n",
    "\n",
    "# agent = build_agent(state_dim=8,\n",
    "#                     action_dim=2)\n",
    "# print(agent)\n",
    "# print(isinstance(agent[\"optimizer\"]), torch.optim.Optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da912cf6",
   "metadata": {},
   "source": [
    "# mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2b70bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_next_trial_id(experiment_name: str = MLFLOW_EXPERIMENT) -> int:\n",
    "    client = MlflowClient()\n",
    "    exp = client.get_experiment_by_name(experiment_name)\n",
    "    if exp is None:\n",
    "        # crea el experimento si no existía\n",
    "        exp_id = client.create_experiment(experiment_name)\n",
    "    else:\n",
    "        exp_id = exp.experiment_id\n",
    "\n",
    "    # cuenta todos los runs en ese experimento\n",
    "    all_runs = client.search_runs(\n",
    "        experiment_ids=[exp_id],\n",
    "        run_view_type=mlflow.tracking.client.ViewType.ALL,\n",
    "        max_results=50000\n",
    "    )\n",
    "    return len(all_runs) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657d7edb",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62284822",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(\n",
    "        env: gym.Env,\n",
    "        n_layers: int,\n",
    "        hidden_size: int,\n",
    "        q_online: torch.nn.Module,\n",
    "        q_target: torch.nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        buffer: ReplayBuffer,\n",
    "        policy: EpsilonGreedyPolicy,\n",
    "        total_steps: int,\n",
    "        learning_rate: float,\n",
    "        gamma: float,\n",
    "        batch_size: int,\n",
    "        experiment_name: str = MLFLOW_EXPERIMENT):\n",
    "    \"\"\"\n",
    "    Esta función implementa la lógica de un paso de entrenamiento\n",
    "    \"\"\"\n",
    "    #cerrar cualquier run activo\n",
    "    try:\n",
    "        if mlflow.active_run() is not None:\n",
    "            mlflow.end_run()\n",
    "    except MlflowException:\n",
    "        pass\n",
    "    \n",
    "    mlflow.set_experiment(experiment_name) #crear experimento\n",
    "    #consultar nº del siguiente trial\n",
    "    trial_id = get_next_trial_id(experiment_name)\n",
    "    run_name = f\"trial-{trial_id}\"\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name): #abrir run\n",
    "        #Registro hiperparámetros\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"gamma\", gamma)\n",
    "        mlflow.log_param(\"n_layers\", n_layers)\n",
    "        mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "        mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "\n",
    "        mlflow.log_param(\"training_ratio\", TRAINING_RATIO)\n",
    "        mlflow.log_param(\"tau\", TAU)\n",
    "        mlflow.log_param(\"max_episodes\", MAX_EPISODES)\n",
    "        mlflow.log_param(\"max_steps_per_ep\", MAX_STEPS_PER_EPISODE)\n",
    "    \n",
    "        episode_count = 0\n",
    "        running_reward_train = None #promedio de recompensas historico suavizado por EMA_RATIO\n",
    "        running_reward_eval = None #promedio de recompensas historico de validacion suavizado por EMA_RATIO\n",
    "        historic_reward = []\n",
    "        best_eval_reward = -float('inf')\n",
    "    \n",
    "        for episode in range(1, MAX_EPISODES + 1):\n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        \n",
    "            episode_reward = 0.0 #suma de todas las recompensas obtenidas en un único episodio (actual)\n",
    "\n",
    "            for time_step in range(1, MAX_STEPS_PER_EPISODE + 1):\n",
    "                #seleccionar accion\n",
    "                action = policy.select_action(state)\n",
    "\n",
    "                #realizar accion\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "                \n",
    "                #actualizar running_reward\n",
    "                episode_reward += reward\n",
    "\n",
    "                #guardar experiencia en el buffer\n",
    "                buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "                ###TRAIN MODEL###\n",
    "                #heuristica: si el buffer tiene suficientes experiencias entrenamos\n",
    "                if len(buffer) >= 2 * batch_size and time_step % TRAINING_RATIO == 0:\n",
    "                    #samplear 32 experiencias del buffer según su error\n",
    "                    samples = buffer.sample(batch_size=batch_size)\n",
    "\n",
    "                    #entrenar el modelo: devuelve error cuadratico medio\n",
    "                    loss = optimize_model(samples=samples,\n",
    "                                        q_model=q_online,\n",
    "                                        t_model=q_target,\n",
    "                                        optimizer=optimizer,\n",
    "                                        gamma = gamma,\n",
    "                                        batch_size = batch_size)\n",
    "\n",
    "                    #transferencia de pesos: solo se modifican un %\n",
    "                    learning_transfer(q_online, q_target, TAU)\n",
    "\n",
    "                #actualizar estado\n",
    "                state = next_state\n",
    "                total_steps += 1\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            #evaluacion\n",
    "            mean_eval_rewards, list_eval_rewards = evaluate(\n",
    "                env = env,\n",
    "                state_dim = env.observation_space.shape[0],\n",
    "                action_dim = env.action_space.n,\n",
    "                n_layers = n_layers,\n",
    "                n_neurons = hidden_size,\n",
    "                q_model = q_online,\n",
    "                model_path = None,\n",
    "                num_episodes = 5,\n",
    "                render = False\n",
    "            )\n",
    "\n",
    "            #actualizar medias moviles de evaluacion\n",
    "            running_reward_eval = (\n",
    "                mean_eval_rewards\n",
    "                if running_reward_eval is None\n",
    "                else EMA_RATIO * mean_eval_rewards + (1 - EMA_RATIO) * running_reward_eval\n",
    "            )\n",
    "            running_reward_train = (\n",
    "                episode_reward\n",
    "                if running_reward_train is None\n",
    "                else EMA_RATIO * episode_reward + (1 - EMA_RATIO) * running_reward_train\n",
    "            )\n",
    "\n",
    "            historic_reward.append(running_reward_train)\n",
    "\n",
    "            if mean_eval_rewards > best_eval_reward:\n",
    "                best_eval_reward = mean_eval_rewards\n",
    "                ckpt_path = f\"models/checkpoints/checkpoint_trial{trial_id}.pth\"\n",
    "                torch.save(q_online.state_dict(),ckpt_path)\n",
    "                mlflow.log_artifact(ckpt_path, artifact_path=\"checkpoints\") #subir a MLflow todos los checkpoints\n",
    "                mlflow.pytorch.log_model(q_online, artifact_path = \"best_model\", registered_model_name=f\"lunar-lander-trial-{trial_id}\")\n",
    "                \n",
    "                torch.save(q_online.state_dict(), \"models/best_model/best_model.pt\") #en local solo el mejor modelo\n",
    "\n",
    "            episode_count += 1\n",
    "\n",
    "            #log de métricas\n",
    "            mlflow.log_metric(\"episode_count\", episode_count, step=episode)\n",
    "            mlflow.log_metric(\"train_reward\", episode_reward, step=episode)\n",
    "            mlflow.log_metric(\"running_train_reward\", running_reward_train, step=episode)\n",
    "            mlflow.log_metric(\"eval_reward\", mean_eval_rewards, step=episode)\n",
    "            mlflow.log_metric(\"running_eval_reward\", running_reward_eval, step=episode)\n",
    "            mlflow.log_metric(\"epsilon\", policy.get_epsilon(), step=episode)\n",
    "            if \"loss\" in locals():\n",
    "                mlflow.log_metric(\"loss\", loss, step=episode)\n",
    "\n",
    "            #mostrar logs\n",
    "            if episode_count % EP == 0 and 'loss' in locals(): #'loss' in locals(): so existe loss (si se ha entrenado el modelo en este episodio)\n",
    "                \n",
    "                #mostrar info de entrenamiento\n",
    "                template = (\n",
    "                    \"[Train] Episode {}/{MAX}  |  \"\n",
    "                    \"Running Eval Reward: {:.2f}  |  \"\n",
    "                    \"ε: {:.3f}  |  \"\n",
    "                    \"Loss: {:.2f} | \"\n",
    "                    \"Steps: {:.2f} | \"\n",
    "                ).replace(\"{MAX}\", str(MAX_EPISODES))\n",
    "                print(template.format(\n",
    "                    episode_count,\n",
    "                    running_reward_eval,\n",
    "                    policy.get_epsilon(),\n",
    "                    loss,\n",
    "                    time_step,\n",
    "\n",
    "                ))\n",
    "                #mostrar info de evaluacion\n",
    "                for idx, r in enumerate(list_eval_rewards, start=1):\n",
    "                    print(f\"[Eval] Episodio {idx}/5 → Recompensa: {r:.2f}\")\n",
    "                print(f\"[Eval] Recompensa media en 5 episodios: {mean_eval_rewards:.2f}\\n\")\n",
    "\n",
    "\n",
    "            #condicion para asegurarse de que se ha resuelto la tarea\n",
    "            if running_reward_eval > STOPPING_REWARD_CRITERIA:\n",
    "                print(\"Solved at episode {}!\".format(episode_count))\n",
    "                break\n",
    "\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    return q_online, best_eval_reward, total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fdd4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    #hiperparámetros\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", LEARNING_RATES[0], LEARNING_RATES[1], log = True)\n",
    "    gamma = trial.suggest_float(\"gamma\",          GAMMAS[0],           GAMMAS[1])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", BATCH_SIZES)\n",
    "    n_layers   = trial.suggest_categorical(\"n_layers\",   N_LAYERS)\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", N_NEURONS)\n",
    "\n",
    "    #construir agente con hiperparámetros\n",
    "    agent = build_agent(\n",
    "    state_dim = state_dim,\n",
    "    action_dim = action_dim,\n",
    "    n_layers = n_layers,\n",
    "    n_neurons = hidden_size,\n",
    "    learning_rate = learning_rate\n",
    "    )\n",
    "    \n",
    "    q_online    = agent[\"q_online\"]\n",
    "    q_target    = agent[\"q_target\"]\n",
    "    optimizer   = agent[\"optimizer\"]\n",
    "    buffer      = agent[\"buffer\"]\n",
    "    policy      = agent[\"policy\"]\n",
    "    total_steps = agent[\"total_steps\"]\n",
    "\n",
    "    #entrenamiento\n",
    "    q_model, eval_best_reward, _ = train(\n",
    "        env = env,\n",
    "        n_layers = n_layers,\n",
    "        hidden_size = hidden_size,\n",
    "        q_online = q_online,\n",
    "        q_target = q_target,\n",
    "        optimizer = optimizer,\n",
    "        buffer = buffer,\n",
    "        policy = policy,\n",
    "        total_steps = total_steps,\n",
    "        learning_rate = learning_rate,\n",
    "        gamma = gamma,\n",
    "        batch_size = batch_size,\n",
    "        experiment_name = MLFLOW_EXPERIMENT\n",
    "    )\n",
    "\n",
    "    return eval_best_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab00a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 18:48:09,387] A new study created in memory with name: no-name-2558d286-e7d1-41b7-a137-f29489c1112e\n",
      "2025/06/11 18:48:10 INFO mlflow.tracking.fluent: Experiment with name 'LunarLander_DQN' does not exist. Creating a new experiment.\n",
      "2025/06/11 18:48:10 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/06/11 18:48:14 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[31m2025/06/11 18:48:14 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Successfully registered model 'lunar-lander-trial-1'.\n",
      "Created version '1' of model 'lunar-lander-trial-1'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Episode 2/10  |  Running Eval Reward: -448.36  |  ε: 0.999  |  Loss: 1.68 | Steps: 86.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -555.81\n",
      "[Eval] Episodio 2/5 → Recompensa: -669.01\n",
      "[Eval] Episodio 3/5 → Recompensa: -615.87\n",
      "[Eval] Episodio 4/5 → Recompensa: -675.18\n",
      "[Eval] Episodio 5/5 → Recompensa: -600.44\n",
      "[Eval] Recompensa media en 5 episodios: -623.26\n",
      "\n",
      "[Train] Episode 4/10  |  Running Eval Reward: -457.92  |  ε: 0.999  |  Loss: 7.09 | Steps: 91.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -707.10\n",
      "[Eval] Episodio 2/5 → Recompensa: -652.74\n",
      "[Eval] Episodio 3/5 → Recompensa: -391.91\n",
      "[Eval] Episodio 4/5 → Recompensa: -863.16\n",
      "[Eval] Episodio 5/5 → Recompensa: -3382.06\n",
      "[Eval] Recompensa media en 5 episodios: -1199.39\n",
      "\n",
      "[Train] Episode 6/10  |  Running Eval Reward: -464.66  |  ε: 0.998  |  Loss: 4.21 | Steps: 127.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -1674.28\n",
      "[Eval] Episodio 2/5 → Recompensa: -674.94\n",
      "[Eval] Episodio 3/5 → Recompensa: -574.98\n",
      "[Eval] Episodio 4/5 → Recompensa: -1453.93\n",
      "[Eval] Episodio 5/5 → Recompensa: -437.53\n",
      "[Eval] Recompensa media en 5 episodios: -963.13\n",
      "\n",
      "[Train] Episode 8/10  |  Running Eval Reward: -471.64  |  ε: 0.998  |  Loss: 2.09 | Steps: 56.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -1131.44\n",
      "[Eval] Episodio 2/5 → Recompensa: -1904.27\n",
      "[Eval] Episodio 3/5 → Recompensa: -614.30\n",
      "[Eval] Episodio 4/5 → Recompensa: -678.07\n",
      "[Eval] Episodio 5/5 → Recompensa: -557.86\n",
      "[Eval] Recompensa media en 5 episodios: -977.19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 18:48:17,138] Trial 0 finished with value: -446.59731647711925 and parameters: {'learning_rate': 0.0006076509717845676, 'gamma': 0.9507895883722522, 'batch_size': 16, 'n_layers': 3, 'hidden_size': 128}. Best is trial 0 with value: -446.59731647711925.\n",
      "2025/06/11 18:48:17 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Episode 10/10  |  Running Eval Reward: -475.63  |  ε: 0.997  |  Loss: 610.16 | Steps: 72.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -928.03\n",
      "[Eval] Episodio 2/5 → Recompensa: -550.88\n",
      "[Eval] Episodio 3/5 → Recompensa: -656.90\n",
      "[Eval] Episodio 4/5 → Recompensa: -492.59\n",
      "[Eval] Episodio 5/5 → Recompensa: -626.72\n",
      "[Eval] Recompensa media en 5 episodios: -651.02\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/11 18:48:20 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[31m2025/06/11 18:48:20 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'lunar-lander-trial-2' already exists. Creating a new version of this model...\n",
      "Created version '3' of model 'lunar-lander-trial-2'.\n",
      "2025/06/11 18:48:20 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/06/11 18:48:24 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[31m2025/06/11 18:48:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'lunar-lander-trial-2' already exists. Creating a new version of this model...\n",
      "Created version '4' of model 'lunar-lander-trial-2'.\n",
      "2025/06/11 18:48:24 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/06/11 18:48:27 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[31m2025/06/11 18:48:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'lunar-lander-trial-2' already exists. Creating a new version of this model...\n",
      "Created version '5' of model 'lunar-lander-trial-2'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Episode 4/10  |  Running Eval Reward: -427.23  |  ε: 0.999  |  Loss: 83.72 | Steps: 129.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -547.91\n",
      "[Eval] Episodio 2/5 → Recompensa: -430.36\n",
      "[Eval] Episodio 3/5 → Recompensa: -652.58\n",
      "[Eval] Episodio 4/5 → Recompensa: -552.91\n",
      "[Eval] Episodio 5/5 → Recompensa: -435.61\n",
      "[Eval] Recompensa media en 5 episodios: -523.88\n",
      "\n",
      "[Train] Episode 6/10  |  Running Eval Reward: -431.41  |  ε: 0.998  |  Loss: 80.80 | Steps: 103.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -691.25\n",
      "[Eval] Episodio 2/5 → Recompensa: -412.93\n",
      "[Eval] Episodio 3/5 → Recompensa: -761.95\n",
      "[Eval] Episodio 4/5 → Recompensa: -476.65\n",
      "[Eval] Episodio 5/5 → Recompensa: -443.02\n",
      "[Eval] Recompensa media en 5 episodios: -557.16\n",
      "\n",
      "[Train] Episode 8/10  |  Running Eval Reward: -438.54  |  ε: 0.998  |  Loss: 170.02 | Steps: 77.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -438.87\n",
      "[Eval] Episodio 2/5 → Recompensa: -681.49\n",
      "[Eval] Episodio 3/5 → Recompensa: -1162.81\n",
      "[Eval] Episodio 4/5 → Recompensa: -886.89\n",
      "[Eval] Episodio 5/5 → Recompensa: -717.50\n",
      "[Eval] Recompensa media en 5 episodios: -777.51\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 18:48:29,573] Trial 1 finished with value: -338.77427987885375 and parameters: {'learning_rate': 0.00041844637764903357, 'gamma': 0.9183833332238363, 'batch_size': 128, 'n_layers': 1, 'hidden_size': 64}. Best is trial 1 with value: -338.77427987885375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Episode 10/10  |  Running Eval Reward: -445.16  |  ε: 0.997  |  Loss: 7.29 | Steps: 121.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -1154.51\n",
      "[Eval] Episodio 2/5 → Recompensa: -552.07\n",
      "[Eval] Episodio 3/5 → Recompensa: -773.91\n",
      "[Eval] Episodio 4/5 → Recompensa: -1478.88\n",
      "[Eval] Episodio 5/5 → Recompensa: -827.01\n",
      "[Eval] Recompensa media en 5 episodios: -957.28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/11 18:48:29 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/06/11 18:48:33 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[31m2025/06/11 18:48:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'lunar-lander-trial-3' already exists. Creating a new version of this model...\n",
      "Created version '5' of model 'lunar-lander-trial-3'.\n",
      "2025/06/11 18:48:33 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/06/11 18:48:36 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[31m2025/06/11 18:48:36 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'lunar-lander-trial-3' already exists. Creating a new version of this model...\n",
      "Created version '6' of model 'lunar-lander-trial-3'.\n",
      "2025/06/11 18:48:37 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/06/11 18:48:40 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[31m2025/06/11 18:48:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'lunar-lander-trial-3' already exists. Creating a new version of this model...\n",
      "Created version '7' of model 'lunar-lander-trial-3'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Episode 4/10  |  Running Eval Reward: -904.59  |  ε: 0.999  |  Loss: 8.67 | Steps: 73.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -1119.46\n",
      "[Eval] Episodio 2/5 → Recompensa: -663.82\n",
      "[Eval] Episodio 3/5 → Recompensa: -839.41\n",
      "[Eval] Episodio 4/5 → Recompensa: -1620.10\n",
      "[Eval] Episodio 5/5 → Recompensa: -640.96\n",
      "[Eval] Recompensa media en 5 episodios: -976.75\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/11 18:48:41 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/06/11 18:48:44 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[31m2025/06/11 18:48:44 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'lunar-lander-trial-3' already exists. Creating a new version of this model...\n",
      "Created version '8' of model 'lunar-lander-trial-3'.\n",
      "2025/06/11 18:48:45 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Episode 6/10  |  Running Eval Reward: -898.35  |  ε: 0.998  |  Loss: 139.00 | Steps: 63.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -754.51\n",
      "[Eval] Episodio 2/5 → Recompensa: -648.18\n",
      "[Eval] Episodio 3/5 → Recompensa: -485.71\n",
      "[Eval] Episodio 4/5 → Recompensa: -624.28\n",
      "[Eval] Episodio 5/5 → Recompensa: -771.70\n",
      "[Eval] Recompensa media en 5 episodios: -656.88\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/11 18:48:48 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[31m2025/06/11 18:48:48 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'lunar-lander-trial-3' already exists. Creating a new version of this model...\n",
      "Created version '9' of model 'lunar-lander-trial-3'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Episode 8/10  |  Running Eval Reward: -889.02  |  ε: 0.998  |  Loss: 101.18 | Steps: 117.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -691.72\n",
      "[Eval] Episodio 2/5 → Recompensa: -612.85\n",
      "[Eval] Episodio 3/5 → Recompensa: -633.63\n",
      "[Eval] Episodio 4/5 → Recompensa: -630.81\n",
      "[Eval] Episodio 5/5 → Recompensa: -300.60\n",
      "[Eval] Recompensa media en 5 episodios: -573.92\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 18:48:49,585] Trial 2 finished with value: -284.0742240334569 and parameters: {'learning_rate': 0.0043973237088562375, 'gamma': 0.984575961988907, 'batch_size': 128, 'n_layers': 2, 'hidden_size': 128}. Best is trial 2 with value: -284.0742240334569.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Episode 10/10  |  Running Eval Reward: -883.40  |  ε: 0.997  |  Loss: 32.48 | Steps: 68.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -495.23\n",
      "[Eval] Episodio 2/5 → Recompensa: -595.18\n",
      "[Eval] Episodio 3/5 → Recompensa: -634.67\n",
      "[Eval] Episodio 4/5 → Recompensa: -681.16\n",
      "[Eval] Episodio 5/5 → Recompensa: -614.73\n",
      "[Eval] Recompensa media en 5 episodios: -604.19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/11 18:48:49 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/06/11 18:48:53 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[31m2025/06/11 18:48:53 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'lunar-lander-trial-4' already exists. Creating a new version of this model...\n",
      "Created version '6' of model 'lunar-lander-trial-4'.\n",
      "2025/06/11 18:48:53 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/06/11 18:48:57 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[31m2025/06/11 18:48:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'lunar-lander-trial-4' already exists. Creating a new version of this model...\n",
      "Created version '7' of model 'lunar-lander-trial-4'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Episode 2/10  |  Running Eval Reward: -134.39  |  ε: 0.999  |  Loss: 2.25 | Steps: 132.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -167.13\n",
      "[Eval] Episodio 2/5 → Recompensa: 13.44\n",
      "[Eval] Episodio 3/5 → Recompensa: -36.78\n",
      "[Eval] Episodio 4/5 → Recompensa: -190.27\n",
      "[Eval] Episodio 5/5 → Recompensa: -123.77\n",
      "[Eval] Recompensa media en 5 episodios: -100.90\n",
      "\n",
      "[Train] Episode 4/10  |  Running Eval Reward: -143.15  |  ε: 0.999  |  Loss: 5.41 | Steps: 57.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -862.89\n",
      "[Eval] Episodio 2/5 → Recompensa: -483.58\n",
      "[Eval] Episodio 3/5 → Recompensa: -760.34\n",
      "[Eval] Episodio 4/5 → Recompensa: -339.63\n",
      "[Eval] Episodio 5/5 → Recompensa: -423.77\n",
      "[Eval] Recompensa media en 5 episodios: -574.04\n",
      "\n",
      "[Train] Episode 6/10  |  Running Eval Reward: -150.78  |  ε: 0.998  |  Loss: 2.15 | Steps: 104.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -643.85\n",
      "[Eval] Episodio 2/5 → Recompensa: -598.55\n",
      "[Eval] Episodio 3/5 → Recompensa: -423.37\n",
      "[Eval] Episodio 4/5 → Recompensa: -490.86\n",
      "[Eval] Episodio 5/5 → Recompensa: -720.16\n",
      "[Eval] Recompensa media en 5 episodios: -575.36\n",
      "\n",
      "[Train] Episode 8/10  |  Running Eval Reward: -160.22  |  ε: 0.997  |  Loss: 12.82 | Steps: 100.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -531.17\n",
      "[Eval] Episodio 2/5 → Recompensa: -707.12\n",
      "[Eval] Episodio 3/5 → Recompensa: -747.98\n",
      "[Eval] Episodio 4/5 → Recompensa: -387.60\n",
      "[Eval] Episodio 5/5 → Recompensa: -853.25\n",
      "[Eval] Recompensa media en 5 episodios: -645.43\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 18:48:58,811] Trial 3 finished with value: -100.9025930578072 and parameters: {'learning_rate': 0.004248729132550595, 'gamma': 0.9822112865288861, 'batch_size': 16, 'n_layers': 3, 'hidden_size': 16}. Best is trial 3 with value: -100.9025930578072.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Episode 10/10  |  Running Eval Reward: -168.93  |  ε: 0.997  |  Loss: 6.78 | Steps: 91.00 | \n",
      "[Eval] Episodio 1/5 → Recompensa: -1020.33\n",
      "[Eval] Episodio 2/5 → Recompensa: -512.34\n",
      "[Eval] Episodio 3/5 → Recompensa: -516.38\n",
      "[Eval] Episodio 4/5 → Recompensa: -660.24\n",
      "[Eval] Episodio 5/5 → Recompensa: -484.05\n",
      "[Eval] Recompensa media en 5 episodios: -638.67\n",
      "\n",
      "Mejores params: {'learning_rate': 0.004248729132550595, 'gamma': 0.9822112865288861, 'batch_size': 16, 'n_layers': 3, 'hidden_size': 16}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(), #usar logica bayesiana para escoger hiperparametros\n",
    "    pruner=optuna.pruners.MedianPruner() #parar entrenamiento si cada X épocas no se supera la mediana de los anteriores\n",
    ")\n",
    "study.optimize(objective, n_trials = OPTUNA_TRIALS)\n",
    "print(\"Mejores params:\", study.best_params)\n",
    "\n",
    "with open(\"models/best_model/best_params.json\", \"w\") as f:\n",
    "    json.dump(study.best_params, f, indent=4)\n",
    "\n",
    "fig = vis.plot_parallel_coordinate(study, params=list(study.best_params.keys()))\n",
    "fig.write_image(\"models/best_model/parallel_coordinate.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LUNAR_LANDER_CONDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
